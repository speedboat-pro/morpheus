{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f452381-999f-45fe-9fa3-708ce88db54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#10 Powerful Features to Simplify Semi-structured Data Management in the Databricks Lakehouse\n",
    "## - Ingest and query complex JSON data like a pro with Delta Lake and SQL\n",
    "Ingesting and querying complex JSON files with semi-structured data can be hard but Auto Loader and Delta Lake make it easy. JSON data is very flexible, which makes it powerful, but with this flexibility come issues that can make it difficult to ingest and query such as:\n",
    "\n",
    "* It’s tedious and fragile to have to define a schema of the JSON that you plan to ingest.\n",
    "* The schema can change over time and you need to be able to handle those changes automatically.\n",
    "* Computers don’t always pick the correct schema for your data and you need a way to hint at the correct format.\n",
    "* Often data engineers have no control of upstream data sources generating the semi-structured data. Column name may be upper or lower case but denotes the same column, data type sometimes changes and you may not want to completely rewrite the already ingested data in delta lake.\n",
    "* You may not want to do the upfront work of flattening out JSON files and extracting every single column and doing so may make the data very hard to use.\n",
    "\n",
    "In this notebook, we will show you what features make working with JSON at scale simple. Below is an Incremental ETL architecture, the left-hand side represents continuous and scheduled ingest and we will discuss how to do both with Auto Loader. After the JSON is ingested into a bronze Delta Lake table, we will discuss the features that make it easy to query complex and semi-structured data types common in JSON data. We will use sales order data to demonstrate how to easily ingest JSON. The nested JSON sales order data sets get complex quickly. In your industry, data may be different or even more complex. Whether your data looks like this or not, the problems above stay the same.\n",
    "\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2021/07/get-start-delta-blog-img-1.png\" width=1000>\n",
    "\n",
    "##NOTE: Databricks Runtime 9.1 and above is needed to run this notebook\n",
    "##NOTE: Run each Cmd one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2877a1c-5dfb-4bd1-bc44-0f3b7460f240",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up object storage locations"
    }
   },
   "outputs": [],
   "source": [
    "username = spark.sql(\"select current_user()\").collect()[0][0]\n",
    "userPrefix = username.split(\"@\")[0].replace(\".\", \"\")\n",
    "# base object storage location path derived from the user's name\n",
    "basePath = \"/tmp/\" + username + \"/autoloaderDemo\"\n",
    "# various object storage locations used in the demo\n",
    "landingZoneLocation = basePath + \"/landingZone\"\n",
    "schemaLocation = basePath + \"/schemaStore\"\n",
    "bronzeTableLocation = basePath + \"/datastore/bronzeTbl\" \n",
    "bronzeCheckPointLocation = basePath + \"/datastore/bronzeCheckpoint\"\n",
    "\n",
    "spark.conf.set(\"c.bronzeTablePath\", \"dbfs:\" + bronzeTableLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7f2762-cd4b-44af-a6b1-c8a53a9ff13c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Break up the data into chunks"
    }
   },
   "outputs": [],
   "source": [
    "# Ths sales order retail data is broken in chunks so that the parts can be used to write out to the landing zone so that Auto Loader can consume them at the correct time\n",
    "dfBase = spark.read.json(\"/databricks-datasets/retail-org/sales_orders/\")\n",
    "dfIngest1 = dfBase.where(dfBase.customer_id <= 10000000)\n",
    "dfIngest2 = dfBase.where(dfBase.customer_id.between(10000000, 20000000))\n",
    "dfIngest3 = dfBase.where(dfBase.customer_id >= 20000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f80690-bd6c-4f91-8c4c-48d4081e505f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write 1st data file to landing zone"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment next 2 rows if you would like to rerun from the beginning\n",
    "spark.sql(\"DROP TABLE IF EXISTS autoloaderBronzeTable\")\n",
    "dbutils.fs.rm(basePath, True)\n",
    "\n",
    "dfIngest1.write.json(landingZoneLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31944bde-bfb8-4a9c-b455-3ed051745c26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Continuous ingest using Auto Loader with schema inference and hints"
    }
   },
   "outputs": [],
   "source": [
    "# We use the inferColumnTypes option for schema inference and the schemaHints option for the hints\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", schemaLocation) \\\n",
    "  .option(\"cloudFiles.format\", \"json\") \\\n",
    "  .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "  .option(\"cloudFiles.schemaHints\", \"clicked_items string, ordered_products.element.promotion_info string\") \\\n",
    "  .load(landingZoneLocation)\n",
    "\n",
    "# We use the mergeSchema option so that the schema can change over time and the checkpointLocation is used to store the state of the stream\n",
    "df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .option(\"checkpointLocation\", bronzeCheckPointLocation) \\\n",
    "  .start(bronzeTableLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e2045a-1214-4e1e-b9b0-7e5f1cb0c45a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display the data written to the Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "# Sleeps for 10 seconds so that there is enough time to set up the stream and write to the table before displaying\n",
    "import time\n",
    "time.sleep(10) \n",
    "df = spark.read.format(\"delta\").load(bronzeTableLocation)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9edeb5-d960-4c6a-96fa-24884bf24126",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write 2nd file to landing zone and add a nested column - as you can see, its stops the stream above"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "# add a new nested column \"fulfillment_days\" to the 2nd dataframe\n",
    "dfIngest2 = dfIngest2.withColumn(\n",
    "  \"fulfillment_days\", f.struct(\\\n",
    "  f.round(f.rand(), 2).alias(\"picking\"), \\\n",
    "  f.round((f.rand() * 2), 2).cast(\"string\").alias(\"packing\"), \\\n",
    "  f.struct(f.lit(\"air\").alias(\"type\"),f.round((f.rand() * 5), 2).alias(\"days\")).alias(\"shipping\")))\n",
    "  \n",
    "# append the 2nd JSON file to the landin zone\n",
    "# since the new data frame has a new column, the schema evolves which stops the query and can be started up right away\n",
    "dfIngest2.write.mode(\"append\").json(landingZoneLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0edb28e5-02ef-40d6-8831-56276678a814",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scheduled ingest that turns itself off using Auto Loader with schema evolution and a new hint"
    }
   },
   "outputs": [],
   "source": [
    "# Added a new hint for the new column fulfillment_days\n",
    "# As you can see from the dataframe below, the new column is accounted for in the stream\n",
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", schemaLocation) \\\n",
    "  .option(\"cloudFiles.format\", \"json\") \\\n",
    "  .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "  .option(\"cloudFiles.schemaHints\", \"clicked_items string, ordered_products.element.promotion_info string, fulfillment_days string\") \\\n",
    "  .load(landingZoneLocation)\n",
    "\n",
    "# the trigger, \"once=True\" is what turns this version from a continuous stream to a scheduled one that turns itself off when it is finished\n",
    "df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .trigger(once=True) \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .option(\"checkpointLocation\", bronzeCheckPointLocation) \\\n",
    "  .start(bronzeTableLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f68daf-3a40-490c-b383-7677a2c8322a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = dbutils.widgets.get(\"schema\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4bc2f5-db61-4634-a99c-707947451f39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE ${catalog}.${schema};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74403b6-ba5a-4b7f-902e-39463c053b73",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Now that the data is in a delta table use SQL to manipulate the complex json structure"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS autoloaderBronzeTable\n",
    "LOCATION '${c.bronzeTablePath}';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1f8894-6a90-4ab5-be1f-08f62f7be282",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Semi-structured query - Extract top level and nested data. Also, casting those values"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT fulfillment_days, fulfillment_days:picking, \n",
    "  fulfillment_days:packing::double, fulfillment_days:shipping.days\n",
    "FROM autoloaderBronzeTable\n",
    "WHERE fulfillment_days IS NOT NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d83174d-afa1-4ba3-a849-3abac9287d11",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Semi-structured query - extract all and specific data from arrays. Also casting array values"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *, reduce(all_click_count_array, 0, (acc, value) -> acc + value) as sum\n",
    "FROM (\n",
    "  SELECT order_number, clicked_items:[*][1] as all_click_counts, \n",
    "    from_json(clicked_items:[*][1], 'ARRAY<STRING>')::ARRAY<INT> as all_click_count_array\n",
    "  FROM autoloaderBronzeTable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1e24e9-3737-4981-83b9-8a2482853ffd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Complex aggregation in SQL with nested columns using Explode and dot notation"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT order_date, ordered_products_explode.name  as product_name, \n",
    "  SUM(ordered_products_explode.qty) as qantity\n",
    "FROM (\n",
    "  SELECT DATE(from_unixtime(order_datetime)) as order_date, \n",
    "    EXPLODE(ordered_products) as ordered_products_explode\n",
    "  FROM autoloaderBronzeTable\n",
    "  WHERE DATE(from_unixtime(order_datetime)) is not null\n",
    "  )\n",
    "GROUP BY order_date, ordered_products_explode.name\n",
    "ORDER BY order_date, ordered_products_explode.name"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4380539275846156,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "10 Powerful Features to Simplify Semi-structured Data Management in the Databricks Lakehouse",
   "widgets": {
    "catalog": {
     "currentValue": "hive_metastore",
     "nuid": "5185a860-21bd-4443-8bd2-0bd14f071095",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "hive_metastore",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "hive_metastore",
      "label": "",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "labuser9271204_1739891266_5yyq_da_adewd_streaming_etl_lookup",
     "nuid": "f5596524-24ea-42a4-b19e-9d15236aefc1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "labuser9271204_1739891266_5yyq_da_adewd_streaming_etl_lookup",
      "label": "",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "labuser9271204_1739891266_5yyq_da_adewd_streaming_etl_lookup",
      "label": "",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
