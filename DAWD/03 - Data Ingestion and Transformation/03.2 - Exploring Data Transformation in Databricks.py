# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC <div style="text-align: center; line-height: 0; padding-top: 9px;">
# MAGIC   <img src="https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png" alt="Databricks Learning">
# MAGIC </div>
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC #Demo: Exploring Data Transformation in Databricks
# MAGIC
# MAGIC This notebook demonstrates the **Medallion Architecture** for data transformation, using **Materialized Views (MV)** and **Streaming Tables (ST)** in SQL. The pipeline progresses through the **Bronze**, **Silver**, and **Gold** layers, showcasing how to build efficient data pipelines in Databricks.
# MAGIC
# MAGIC **Learning Objectives**
# MAGIC
# MAGIC By the end of this notebook, you should be able to:
# MAGIC - Understand the **Medallion Architecture** and its role in data pipelines.
# MAGIC - Declare and configure **DLT** pipelines for automated data processing.
# MAGIC - Use **Materialized Views** and **Streaming Tables** for different data transformation workloads.
# MAGIC - Enforce data quality with **constraints** in DLT pipelines.
# MAGIC - Explore and analyze tables generated by a DLT pipeline using SQL.

# COMMAND ----------

# MAGIC %md
# MAGIC ## REQUIRED - SELECT CLASSIC COMPUTE
# MAGIC Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.
# MAGIC
# MAGIC Follow these steps to select the classic compute cluster:
# MAGIC 1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.
# MAGIC
# MAGIC 1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:
# MAGIC     - In the drop-down, select **More**.
# MAGIC     - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.
# MAGIC     
# MAGIC **NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:
# MAGIC
# MAGIC 1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.
# MAGIC
# MAGIC 1. Find the triangle icon to the right of your compute cluster name and click it.
# MAGIC 1. Wait a few minutes for the cluster to start.
# MAGIC 1. Once the cluster is running, complete the steps above to select your cluster.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Requirements
# MAGIC
# MAGIC Please review the following requirements before starting the lesson:
# MAGIC
# MAGIC - To run this notebook, you need to use one of the following Databricks runtime(s): `16.3.x-scala2.12`

# COMMAND ----------

# MAGIC %md
# MAGIC ##Classroom Setup
# MAGIC
# MAGIC Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.
# MAGIC <br></br>
# MAGIC
# MAGIC
# MAGIC ```
# MAGIC USE CATALOG dbacademy;
# MAGIC USE SCHEMA dbacademy.<your unique schema name>;
# MAGIC ```
# MAGIC
# MAGIC **NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.

# COMMAND ----------

# MAGIC %run ../Includes/Classroom-Setup-3.2

# COMMAND ----------

# MAGIC %md
# MAGIC **Other Conventions:**
# MAGIC
# MAGIC Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:

# COMMAND ----------

print(f"Username:          {DA.username}")
print(f"Catalog Name:      {DA.catalog_name}")
print(f"Schema Name:       {DA.schema_name}")
print(f"Working Directory: {DA.paths.working_dir}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## A. Tables as Query Results
# MAGIC
# MAGIC DLT adapts standard SQL queries to combine DDL (data definition language) and DML (data manipulation language) into a unified declarative syntax.
# MAGIC
# MAGIC There are two distinct types of persistent tables that can be created with DLT:
# MAGIC
# MAGIC * **Materialized View**  
# MAGIC Materialized views are refreshed according to the update schedule of the pipeline in which theyâ€™re contained. Materialized views are powerful because they can handle any changes in the input. Each time the pipeline updates, query results are recalculated to reflect changes in upstream datasets that might have occurred because of compliance, corrections, aggregations, or general CDC.
# MAGIC
# MAGIC * **Streaming Tables**  
# MAGIC Streaming tables allow you to process a growing dataset, handling each row only once. Because most datasets grow continuously over time, streaming tables are good for most ingestion workloads. Streaming tables are optimal for pipelines that require data freshness and low latency.
# MAGIC
# MAGIC Note that both of these objects are persisted as tables stored with the Delta Lake protocol (providing ACID transactions, versioning, and many other benefits). We'll talk more about the differences between materialized views and streaming tables later in the notebook.
# MAGIC
# MAGIC For both kinds of tables, DLT takes the approach of a slightly modified CTAS (create table as select) statement. Engineers just need to worry about writing queries to transform their data, and DLT handles the rest.
# MAGIC
# MAGIC The basic syntax for a SQL DLT query is:
# MAGIC
# MAGIC **`CREATE OR REFRESH [STREAMING] TABLE table_name`**<br/>
# MAGIC **`AS select_statement`**<br/>

# COMMAND ----------

# MAGIC %md
# MAGIC ## B. Explore Available Raw Files
# MAGIC
# MAGIC Complete the following steps to explore the available raw data files that will be used for the DLT pipeline:
# MAGIC
# MAGIC 1. Navigate to the available catalogs by selecting the catalog icon directly to the left of the notebook (do not select the **Catalog** text in the far left navigation bar).
# MAGIC 1. Expand the **dbacademy > {DA.schema_name} > Volumes**.
# MAGIC 1. Expand the volume that contains your **unique username**.
# MAGIC 1. Expand the **stream-source** directory. Notice that the directory contains three subdirectories: **customers** and **orders**.
# MAGIC 1. Expand each subdirectory. Notice that each contains a JSON file (00.json) with raw data. We will create a DLT pipeline that will ingest the files within this volume to create tables and materialized views for our consumers.

# COMMAND ----------

# MAGIC %md
# MAGIC ## C. DLT Pipeline: Customer Order Pipeline
# MAGIC
# MAGIC Check out how to create **Streaming Tables** and **Materialized Views (MV)** for the Medallion Architecture by reviewing the **Customer Order Pipeline** notebook. Follow these steps:
# MAGIC
# MAGIC 1. Open the [Customer Order Pipeline]($./Pipelines/Customer%20Order%20Pipeline) notebook.
# MAGIC    - Do not attempt to execute the code directly. It is intended to be executed within the context of the DLT pipeline workflow.
# MAGIC    - Examine how **Streaming Tables** and **Materialized Views** are implemented to process data through the **Bronze**, **Silver**, and **Gold** layers.
# MAGIC    - Observe the step-by-step creation and transformation of tables within the Medallion Architecture, including data ingestion, validation, and enrichment techniques.
# MAGIC 1. After reviewing the pipeline notebook and understanding its concepts, close the tab and return to this notebook to proceed with generating the DLT pipeline and completing additional tasks.

# COMMAND ----------

# MAGIC %md
# MAGIC ## D. Generate Pipeline Configuration
# MAGIC DLT pipelines can be written in either SQL or python. In the code cell below, note that we are first going to look at the SQL example. 
# MAGIC
# MAGIC We are going to manually configure a pipeline using the DLT UI. Configuring this pipeline will require parameters unique to a given user. Run the cell to print out values you'll use to configure your pipeline in subsequent steps.

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Set schema for the session
# MAGIC USE SCHEMA ${DA.schema_name};

# COMMAND ----------

pipeline_language = "SQL"

DA.print_pipeline_config(pipeline_language)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Create and Configure a Pipeline
# MAGIC
# MAGIC Complete the following to configure the pipeline.
# MAGIC
# MAGIC Steps:
# MAGIC 1. Open the [Pipelines user interface](/pipelines) (or use the **Workflows** option from the left sidebar and select the Pipelines tab).
# MAGIC 1. Click **Create** in the upper-right corner, and select **ETL pipeline** from the dropdown menu.
# MAGIC 1. Configure the pipeline as specified below. You'll need the values provided in the cell output above for this step.
# MAGIC
# MAGIC | Setting | Instructions |
# MAGIC |--|--|
# MAGIC | Pipeline name | Enter the **Pipeline Name** provided above |
# MAGIC | Cluster | Click on **Serverless** |
# MAGIC | Pipeline mode | Choose **Triggered** |
# MAGIC | Paths | Use the navigator to select or enter the notebook path provided above |
# MAGIC | Storage options | Check for **Unity Catalog** is Enabled  |
# MAGIC | Default catalog | Choose your **Default Catalog** provided above |
# MAGIC | Default schema | Choose the **Default Schema** provided above |
# MAGIC | Configuration | Click **Add Configuration** and input the **Key** and **Value** in the table below|
# MAGIC | Channel | Choose **Current** |
# MAGIC
# MAGIC
# MAGIC | Key                 | Value                                      |
# MAGIC | ------------------- | ------------------------------------------ |
# MAGIC | **`source`** | Enter the **source** provided above |
# MAGIC
# MAGIC <br>
# MAGIC
# MAGIC 4. Click the **Create** button.
# MAGIC 5. Verify that the pipeline mode is set to **Development**.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Check Your Pipeline Configuration
# MAGIC
# MAGIC 1. In the Databricks workspace, open the Pipelines UI.
# MAGIC 1. Select your pipeline configuration.
# MAGIC 1. Review the pipeline configuration settings to ensure they are correctly configured according to the provided instructions.
# MAGIC 1. **Important:** Remove the maintenance cluster if it is currently part of your pipeline configuration. This is required to successfully validate the pipeline configuration. Do this by clicking JSON in the upper-right corner and removing the code related to the maintenance cluster.
# MAGIC 1. Once you've confirmed that the pipeline configuration is set up correctly and the maintenance cluster has been removed, proceed to the next steps for validating and running the pipeline.

# COMMAND ----------

DA.validate_pipeline_config(pipeline_language)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Update the pipeline
# MAGIC
# MAGIC Trigger an update of the pipeline you created by clicking the **Start** button back in the Pipeline user interface.

# COMMAND ----------

# MAGIC %md
# MAGIC ##Querying Tables in the Target Database
# MAGIC
# MAGIC As long as a target database is specified during DLT Pipeline configuration, tables should be available to users throughout your Databricks environment. Let's explore them now. 
# MAGIC
# MAGIC Run the cell below to see the tables registered to the database used so far. The tables were created in the **dbacademy** catalog, within your unique **schema** name.

# COMMAND ----------

# MAGIC %sql
# MAGIC SHOW TABLES

# COMMAND ----------

# MAGIC %md
# MAGIC Note that the view we defined in our pipeline is absent from our tables list.
# MAGIC
# MAGIC Query results from the **`order_silver`** table.

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM order_silver

# COMMAND ----------

# MAGIC %md
# MAGIC Recall that **`orders_bronze`** was defined as a streaming table in DLT, but our results here are static.
# MAGIC
# MAGIC Because DLT uses Delta Lake to store all tables, each time a query is executed, we will always return the most recent version of the table. But queries outside of DLT will return snapshot results from DLT tables, regardless of how they were defined.

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC ###Show Lineage for Delta Tables in Unity Catalog
# MAGIC
# MAGIC
# MAGIC <img src="https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/uc/lineage/uc-lineage-slide.png" style="float:right; margin-left:10px" width="700"/>
# MAGIC
# MAGIC
# MAGIC Unity Catalog captures runtime data lineage for all **table-to-table operations** executed on Databricks clusters or SQL endpoints. Lineage works seamlessly across all languages, including **SQL, Python, Scala, and R**. It can be visualized in **Data Explorer** in near real-time and can also be retrieved programmatically using the **REST API**. 
# MAGIC
# MAGIC **Lineage Granularity Levels**
# MAGIC
# MAGIC Unity Catalog supports data lineage at two levels:
# MAGIC 1. **Table-Level Lineage**:
# MAGIC    - Tracks the flow of data between entire tables.
# MAGIC    - Useful for understanding the broader context of data operations.
# MAGIC
# MAGIC 2. **Column-Level Lineage**:
# MAGIC    - Tracks data transformations at the column level.
# MAGIC    - Ideal for use cases like **GDPR compliance** and tracking sensitive data dependencies.
# MAGIC
# MAGIC **Access Control with Table ACLs**
# MAGIC
# MAGIC Lineage respects the **Table ACLs** (Access Control Lists) defined in Unity Catalog:
# MAGIC - If a user does not have access to a table in the lineage graph, its details will be redacted.
# MAGIC - However, users will still see the presence of upstream or downstream dependencies, ensuring visibility into the flow of data while maintaining security.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC **Benefits of Viewing Lineage**
# MAGIC 1. **End-to-End Data Visibility**:
# MAGIC    - Understand how data flows through the pipeline, from source to final output.
# MAGIC
# MAGIC 2. **Compliance and Governance**:
# MAGIC    - Ensure GDPR compliance by tracking sensitive data dependencies at the column level.
# MAGIC
# MAGIC 3. **Debugging and Optimization**:
# MAGIC    - Identify bottlenecks and optimize transformations for better performance.

# COMMAND ----------

# MAGIC %md
# MAGIC ### Steps to View Lineage in Unity Catalog
# MAGIC
# MAGIC Follow these steps to view the lineage of Delta Tables in Unity Catalog:
# MAGIC
# MAGIC **Step 1: Navigate to the Pipelines**
# MAGIC - Run the following code to generate the pipeline URL.
# MAGIC - Click on the printed **Pipeline URL** to navigate to the pipeline page.
# MAGIC
# MAGIC **Step 2: Select the Materialized View**
# MAGIC - On the pipeline page, locate the materialized view of interest (e.g., `customer_order`).
# MAGIC - Click on the materialized view to open its **Details** tab. 
# MAGIC
# MAGIC **Step 3: Open the Table in Catalog Explorer**
# MAGIC - Under the **Details** tab of the materialized view, locate the table name (e.g., `dbacademy.schem_name.customer_order`).
# MAGIC - Click on the table name to navigate to the **Catalog Explorer**. 
# MAGIC
# MAGIC **Step 4: View Lineage Tab**
# MAGIC - In the Catalog Explorer, select the **Lineage** tab from the menu at the top. 
# MAGIC - This will display a summary of the table's upstream and downstream dependencies.
# MAGIC
# MAGIC **Step 5: Open the Lineage Graph**
# MAGIC - In the **Lineage** tab, locate the **"See Lineage Graph"** button in the top-right corner of the page.
# MAGIC - Click on the button to open the expanded lineage graph. 
# MAGIC
# MAGIC **Step 6: Explore the Lineage Graph**
# MAGIC - The **Lineage Graph** will display the data flow for the table:
# MAGIC    - **Upstream Tables**: Represent the data sources feeding into the pipeline.
# MAGIC    - **Downstream Tables**: Represent the outputs or dependencies created from the table.
# MAGIC - Click on the **`+` icons** to expand the graph and reveal additional details about each connection.

# COMMAND ----------

try:
    # Retrieve workspace URL dynamically from Databricks configurations
    workspace_url = spark.conf.get('spark.databricks.workspaceUrl')
    pipeline_name = f"{DA.schema_name}: Data Transformation Pipeline"
    
    # Retrieve and print the clickable pipeline URL
    pipeline_url = get_pipeline_url(workspace_url, pipeline_name)
    print(f"DLT Pipeline URL: {pipeline_url}")
except ValueError as e:
    print(e)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Conclusion
# MAGIC
# MAGIC In this notebook, we explored data transformation in Databricks using the Medallion Architecture, highlighting the capabilities of DLT to build robust and efficient pipelines. We demonstrated the creation and configuration of pipelines that use Materialized Views (MV) and Streaming Tables (ST) to process and transform data across Bronze, Silver, and Gold layers.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC &copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href="https://www.apache.org/" target="blank">Apache Software Foundation</a>.<br/>
# MAGIC <br/><a href="https://databricks.com/privacy-policy" target="blank">Privacy Policy</a> | 
# MAGIC <a href="https://databricks.com/terms-of-use" target="blank">Terms of Use</a> | 
# MAGIC <a href="https://help.databricks.com/" target="blank">Support</a>
