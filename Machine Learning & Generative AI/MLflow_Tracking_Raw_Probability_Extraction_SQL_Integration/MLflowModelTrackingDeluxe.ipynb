{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b9cae62-1953-4809-ba83-c1996ac21ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa528201-f349-42e2-9127-09bfd16f3ae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Model Tracking with *MLflow*\n",
    "\n",
    "In this demo, we will explore the capabilities of MLflow, a comprehensive framework for the complete machine learning lifecycle. MLflow provides tools for tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\n",
    "\n",
    "In this demo, **we will focus on tracking and logging components of MLflow**. First, we will demonstrate how to track an experiment with MLflow and show various custom logging features including logging parameters, metrics, figures and arbitrary artifacts.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to*;\n",
    "\n",
    "* Manually log parameters, metrics, models, and figures with MLflow tracking.\n",
    "\n",
    "* Review an experiment using the MLflow UI.\n",
    "\n",
    "* Train a model using a Feature Store table as the modeling set.\n",
    "\n",
    "* Log training dataset with model in MLFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7443d138-e4c5-4634-b35d-96c99ef5e770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLFlow with Unity Catalog\n",
    "\n",
    "Databricks has support for MLflow with Unity Catalog (UC) integration and workspace based classic version. Although we won't go into the details of MLflow with UC in this demo, we will enable it. This means **models will be registered to UC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff501a74-1824-492c-b1db-63b6b57e0a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade 'mlflow-skinny[databricks]'\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5983e41-ed92-4c90-98b9-fbee5749a485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#adding parameters for catalog and schema\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b3ddfe-b6df-4474-aba9-a709cb063fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG ${catalog_name};\n",
    "USE SCHEMA ${schema_name};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ebbb35-e900-4c2e-90d0-c95049f9bbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da16d122-6585-4dff-98cb-f535c6582634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Copy the `diabetes_data.csv` file to the `data` Volume in your schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1aa419d-9b3e-470f-bd4b-78392ba6fb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"dbfs:/Volumes/{catalog_name}/{schema_name}/data/diabetes_data.csv\")\n",
    "\n",
    "# create the feature table using the PySpark DataFrame\n",
    "table_name = f\"{catalog_name}.{schema_name}.diabetes_binary\"\n",
    "fe.create_table(\n",
    "    name=table_name,\n",
    "    primary_keys=[\"unique_id\"],\n",
    "    df=df,\n",
    "    description=\"Diabetes Feature Table\",\n",
    "    tags={\"source\": \"silver\", \"format\": \"delta\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b5229e-075b-48a5-bcd0-d28e7e270b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee94619d-f917-4fb4-af6a-8313221f550b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "In this section, we will leverage the Feature Store to load the dataset for our machine learning experiment. Instead of directly reading from a CSV file, we will use the Feature Store setup to create a feature table and then read the data from it. This approach enhances reproducibility and ensures consistency in the datasets used for training and testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d9f88a-c2f9-41aa-9a1b-0ced6b5dda1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "feature_dataset = mlflow.data.load_delta(\n",
    "    table_name = f\"dbacademy.labuser9597408_1742230869.diabetes_binary\", \n",
    "    name = \"diabetes_binary\"\n",
    ")   \n",
    "feature_data_pd = feature_dataset.df.toPandas()\n",
    "# Drop the 'unique_id' column\n",
    "feature_data_pd = feature_data_pd.drop(\"unique_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019e327d-c4be-46cd-8caa-6b0b05c3c32c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(feature_data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3c4656-5efe-49ff-8321-f8a3ea9a786f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert all columns in the DataFrame to the 'double' data type\n",
    "for column in feature_data_pd.columns:\n",
    "    feature_data_pd[column] = feature_data_pd[column].astype(\"double\")\n",
    "\n",
    "# If you want to see the updated types\n",
    "print(feature_data_pd.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cfd33a-8dd1-447a-a80f-fb336c4349ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train / Test Split\n",
    "\n",
    "Before proceeding with model training, it's essential to split the dataset into training and testing sets. This step ensures that the model is trained on one subset of the data and evaluated on an independent subset, providing a reliable estimate of its performance on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e142411-c1ae-4051-b704-1d9d57b52c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {feature_data_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into it's own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = feature_data_pd.drop(labels=target_col, axis=1)\n",
    "y_all = feature_data_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a0deba-5f57-40ac-835d-d8a8138214fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit and Log the Model\n",
    "\n",
    "Now that we have our training and testing sets, let's fit a Decision Tree model to the training data. During this process, we will use MLflow to log various aspects of the model, including parameters, metrics, and the resulting model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe5eb86-f0b9-4813-8a69-1dc5a1a8602f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dtc_params = {\n",
    "  'criterion': 'gini',\n",
    "  'max_depth': 50,\n",
    "  'min_samples_split': 20,\n",
    "  'min_samples_leaf': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e873ec6-1033-4155-9498-7072243f3b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this code, we use MLflow to start a run and log parameters such as the criterion and max_depth of the Decision Tree model. After fitting the model on the training data, we evaluate its performance on the test set and log the accuracy as a metric.\n",
    "\n",
    "**🚨 Important:** MLflow autologging is **enabled by default on Databricks**. This means you don't need to do anything for supported libraries. In the next section, we are disabling it and manually log params, metrics etc. just demonstrate how to do it manually when you need to log any custom model info.\n",
    "\n",
    "**💡 Note: We won't define the `experiment name`, all *runs* generated in this notebook will be logged under the notebook title.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506f2a02-bc8e-4f9f-a5f9-a2ba95029e6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# register models in UC\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42fcb5e-88e8-4889-9f69-eea5515341d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = spark.sql(\"select current_user()\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba5cd7e-b379-4606-bac4-e452b6a3d42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.data\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# set the path for mlflow experiment\n",
    "mlflow.set_experiment(f\"/Users/{username}/Model-Tracking-with-MLflow\")\n",
    "\n",
    "# turn off autologging\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "model_name = f\"{schema_name}.diabetes-model\"\n",
    "\n",
    "# start an MLFlow run\n",
    "with mlflow.start_run(run_name=\"Model Tracking\") as run:\n",
    "  # log the dataset\n",
    "  mlflow.log_input(feature_dataset, context=\"source\")\n",
    "  mlflow.log_input(mlflow.data.from_pandas(X_train, source=feature_dataset.source), context=\"training\")\n",
    "  mlflow.log_input(mlflow.data.from_pandas(X_test, source=feature_dataset.source), context=\"test\")\n",
    "\n",
    "  # log our parameters\n",
    "  mlflow.log_params(dtc_params)\n",
    "\n",
    "  # fit our model\n",
    "  dtc = DecisionTreeClassifier(**dtc_params)\n",
    "  dtc_mdl = dtc.fit(X_train, y_train)\n",
    "\n",
    "  # define model signiture\n",
    "  signature = infer_signature(X_all, y_all)\n",
    "\n",
    "  # log the model\n",
    "  mlflow.sklearn.log_model(\n",
    "    sk_model = dtc_mdl, \n",
    "    artifact_path=\"model-artifacts\",\n",
    "    signature=signature,\n",
    "    registered_model_name=model_name)\n",
    "  \n",
    "  # evaluate on the training set\n",
    "  y_pred = dtc_mdl.predict(X_train)\n",
    "  mlflow.log_metric(\"train_accuracy\", accuracy_score(y_train, y_pred))\n",
    "  mlflow.log_metric(\"train_precision\", precision_score(y_train, y_pred))\n",
    "  mlflow.log_metric(\"train_recall\", recall_score(y_train, y_pred))\n",
    "  mlflow.log_metric(\"train_f1\", f1_score(y_train, y_pred))\n",
    "\n",
    "  # evaluate on the test set\n",
    "  y_pred = dtc_mdl.predict(X_test)\n",
    "  mlflow.log_metric(\"test_accuracy\", accuracy_score(y_test, y_pred))\n",
    "  mlflow.log_metric(\"test_precision\", precision_score(y_test, y_pred))\n",
    "  mlflow.log_metric(\"test_recall\", recall_score(y_test, y_pred))\n",
    "  mlflow.log_metric(\"test_f1\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9defdf-6791-4d01-82db-7248c97863bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point we can access all model details using the **`run.info`** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899f7b55-f596-4874-b6ea-e307c83c57ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba6a66c-775e-49d5-bd8f-ad73c2b71a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log Model Artifacts\n",
    "\n",
    "**In addition to logging parameters, metrics, and the model itself, we can also log artifacts—any files or data relevant to the run.** Let's set up an MLflow client to log artifacts after the run is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88198df9-dcd8-410a-8112-dbb78dfeaaec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.client import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43a8e824-29a7-4e98-aedc-36106cea62ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Log Confusion Matrix\n",
    "\n",
    "The confusion matrix is a useful tool to visualize the classification performance of the model. It provides insights into the true positive, true negative, false positive, and false negative predictions. \n",
    "\n",
    "Let's create the confusion matrix and **log it with MLflow** using **`log_figure`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6fe7d0-c9b6-4bc3-bba3-06562740eae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Computing the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "# Creating a figure object and axes for the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plotting the confusion matrix using the created axes\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Setting the title of the plot\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Now 'fig' can be used with MLFlow's log_figure function\n",
    "client.log_figure(run.info.run_id, figure=fig, artifact_file=\"confusion_matrix.png\")\n",
    "\n",
    "# Showing the plot here for demonstration\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce8b20c-4fe2-41f5-96aa-b85d0f9ef2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Log Feature Importance\n",
    "\n",
    "Now, **let's examine and log the resulting model**. We'll extract and plot the feature importances inferred from the Decision Tree model to understand which data features are most critical for successful prediction.\n",
    "\n",
    "Similar to the previous figure, we will use **`log_figure`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4ae41a-c9f8-4b2d-a831-a039b6574d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Retrieving feature importances\n",
    "feature_importances = dtc_mdl.feature_importances_\n",
    "feature_names = X_train.columns.to_list()\n",
    "\n",
    "# Plotting the feature importances\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "y_pos = np.arange(len(feature_names))\n",
    "ax.bar(y_pos, feature_importances, align='center', alpha=0.7)\n",
    "ax.set_xticks(y_pos)\n",
    "ax.set_xticklabels(feature_names, rotation=45)\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_title('Feature Importances in Decision Tree Classifier')\n",
    "\n",
    "# log to mlflow\n",
    "client.log_figure(run.info.run_id, figure=fig, artifact_file=\"feature_importances.png\")\n",
    "\n",
    "# display here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8b69de3-2850-42f0-a039-65a1666b0b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Log Tree Structure\n",
    "\n",
    "Decision trees make splitting decisions on different features at different critical values, and visualizing the tree structure helps us understand the decision logic. We'll plot the branching tree structure for better interpretation.\n",
    "\n",
    "We can get the tree in text format or as a graph. **To log the text format we will use `log_artifact` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8f259b-c802-41a1-b81f-d6a4bc536dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The fitted DecisionTreeClassifier model has {dtc_mdl.tree_.node_count} nodes and is up to {dtc_mdl.tree_.max_depth} levels deep.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56528e6b-c5c4-4cb1-b3f1-c1eae4604260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is a very large decision tree, printing out the full tree logic, we can see it is vast and sprawling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a043f76-e5b3-478a-b50f-be806075deee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "text_representation = export_text(dtc_mdl, feature_names=feature_names)\n",
    "print(text_representation)\n",
    "\n",
    "# save this to a local file\n",
    "tree_struct_filename = \"tree_structure.txt\"\n",
    "with open(tree_struct_filename,'w') as f:\n",
    "  f.write(text_representation)\n",
    "\n",
    "# log it to mlflow\n",
    "client.log_artifact(run.info.run_id, tree_struct_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "185889dc-9054-43ed-858f-f18724854684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's create a visually better looking version of this tree and log it with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca94acd-9442-4820-afd0-5788d356940f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# plot the tree structure\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "plot_tree(dtc_mdl, \n",
    "          feature_names=feature_names,\n",
    "          max_depth=2,\n",
    "          class_names=['0', '1'], \n",
    "          filled=True,\n",
    "          ax=ax)\n",
    "ax.set_title('Decision Tree Structure')\n",
    "\n",
    "# log it to mlflow\n",
    "client.log_figure(run.info.run_id, fig, \"decision_tree_structure.png\")\n",
    "\n",
    "# display it here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e3b5a1e-5568-4cdf-971d-938bab6aa4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Review the Model via the UI\n",
    "\n",
    "\n",
    "To review the model and its details, follow these step-by-step instructions:\n",
    "\n",
    "+ **Step 1: Go to the \"Experiments\" Section:**\n",
    "  - Click the Experiment icon <img src= \"https://docs.databricks.com/en/_images/experiment.png\" width=10> in the notebook’s right sidebar\n",
    "\n",
    "  - In the Experiment Runs sidebar, click the <img src= \"https://docs.databricks.com/en/_images/external-link.png\" width=10> icon next to the date of the run. The MLflow Run page displays, showing details of the run, including parameters, metrics, tags, and a list of artifacts.\n",
    "\n",
    "  <div style=\"overflow: hidden; width: 200px; height: 200px;\">\n",
    "    <img src=\"https://docs.databricks.com/en/_images/quick-start-nb-experiment.png\" width=1000\">\n",
    "</div>\n",
    "\n",
    "\n",
    "+ **Step 2: Locate Your Experiment:**\n",
    "\n",
    "    - Find the experiment name you specified in your MLflow run.\n",
    "\n",
    "+ **Step 3: Review Run Details:**\n",
    "\n",
    "  - Click on the experiment name to view the runs within that experiment.\n",
    "  - Locate the specific run you want to review.\n",
    "\n",
    "+ **Step 4: Reviewing Artifacts and Metrics:**\n",
    "\n",
    "  - Click on the run to see detailed information.\n",
    "  - Navigate to the \"Artifacts\" tab to view logged artifacts.\n",
    "  - Navigate to the \"Metrics\" tab to view logged metrics.\n",
    "\n",
    "+ **Step 5: Viewing Confusion Matrix Image:**\n",
    "\n",
    "  - If you logged the confusion matrix as an artifact, you can find it in the \"Artifacts\" tab.\n",
    "  - You may find a file named \"confusion_matrix.png\" (or the specified artifact file name).\n",
    "  - Download or view the confusion matrix image.\n",
    "\n",
    "+ **Step 6: View models in the UI:**\n",
    "  - You can find details about the logged model under the <img src = \"https://docs.databricks.com/en/_images/models-icon.png\" width = 20> **Models** tab.\n",
    "  - Look for the model name you specified in your MLflow run (e.g., \"decision_tree_model\").\n",
    "\n",
    "+ **Explore Additional Options:**\n",
    "\n",
    "  - You can explore other tabs and options in the MLflow UI to gather more insights, such as \"Parameters,\" \"Tags,\" and \"Source.\"\n",
    "These instructions will guide you through reviewing and exploring the tracked models using the MLflow UI, providing valuable insights into the experiment results and registered models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5c18d7a-7825-4a0a-9bd9-5323819e8c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Prepare SQL UDF from Registered MLflow Model\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb32cec-97e7-40d3-9c1a-b4a24a19bb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Standard Binary Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd035b7-e3f8-40fe-b311-18c8f4017bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = f\"models:/{model_name}/1\"\n",
    "pyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_uri)\n",
    "spark.udf.register(\"diabetes_apply\", pyfunc_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc16935b-3726-49e0-bceb-93c337ae7a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "WITH inputs AS (\n",
    "  SELECT \n",
    "    CAST(HighBP AS DOUBLE) AS HighBP,\n",
    "    CAST(HighChol AS DOUBLE) AS HighChol,\n",
    "    CAST(CholCheck AS DOUBLE) AS CholCheck,\n",
    "    CAST(BMI AS DOUBLE) AS BMI,\n",
    "    CAST(Smoker AS DOUBLE) AS Smoker,\n",
    "    CAST(Stroke AS DOUBLE) AS Stroke,\n",
    "    CAST(HeartDiseaseorAttack AS DOUBLE) AS HeartDiseaseorAttack,\n",
    "    CAST(PhysActivity AS DOUBLE) AS PhysActivity,\n",
    "    CAST(Fruits AS DOUBLE) AS Fruits,\n",
    "    CAST(Veggies AS DOUBLE) AS Veggies,\n",
    "    CAST(HvyAlcoholConsump AS DOUBLE) AS HvyAlcoholConsump,\n",
    "    CAST(AnyHealthcare AS DOUBLE) AS AnyHealthcare,\n",
    "    CAST(NoDocbcCost AS DOUBLE) AS NoDocbcCost,\n",
    "    CAST(GenHlth AS DOUBLE) AS GenHlth,\n",
    "    CAST(MentHlth AS DOUBLE) AS MentHlth,\n",
    "    CAST(PhysHlth AS DOUBLE) AS PhysHlth,\n",
    "    CAST(DiffWalk AS DOUBLE) AS DiffWalk,\n",
    "    CAST(Sex AS DOUBLE) AS Sex,\n",
    "    CAST(Age AS DOUBLE) AS Age,\n",
    "    CAST(Education AS DOUBLE) AS Education,\n",
    "    CAST(Income AS DOUBLE) AS Income\n",
    "  FROM dbacademy.labuser9597408_1742230869.diabetes_binary\n",
    ")\n",
    "SELECT *, diabetes_apply(struct(*)) AS predictions \n",
    "FROM inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9338d27f-a4c6-48af-b535-ac6846c92b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Raw Probability Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe31d5c-bfd0-47e9-9880-033cc96d8203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS model_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "331017df-669c-4b54-b3c3-a790e28c77de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: you will need to look at the **Artifacts** folder in the model run and copy the `_model.pkl_` file to the `_model_pickle_` volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa43518f-7321-42fc-bda2-461a8e892fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load model from copying to UC volume\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "import pickle\n",
    "\n",
    "with open(f\"/Volumes/{catalog_name}/{schema_name}/model_pickle/model.pkl\",'rb') as f:\n",
    "    raw_model = pickle.load(f)\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "\n",
    "# Define UDF to apply model.predict_proba\n",
    "\n",
    "def predict_proba_udf(*features):\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert tuple to numpy array for LightGBM\n",
    "    X = np.array(features)\n",
    "    \n",
    "    # Get probabilities for class 1\n",
    "    prob = raw_model.predict_proba(X)[:, -1]  # Extract positive class probability\n",
    "\n",
    "    return prob.tolist()  # Convert to list for Spark compatibility\n",
    "\n",
    "# Register UDF in Spark\n",
    "predict_proba_spark_udf = udf(predict_proba_udf, ArrayType(FloatType()))\n",
    "spark.udf.register(\"diabetes_proba\", predict_proba_spark_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b54d7a8f-fabf-4ebb-9c16-0f6990fb096a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH inputs AS (\n",
    "  SELECT \n",
    "    CAST(HighBP AS DOUBLE) AS HighBP,\n",
    "    CAST(HighChol AS DOUBLE) AS HighChol,\n",
    "    CAST(CholCheck AS DOUBLE) AS CholCheck,\n",
    "    CAST(BMI AS DOUBLE) AS BMI,\n",
    "    CAST(Smoker AS DOUBLE) AS Smoker,\n",
    "    CAST(Stroke AS DOUBLE) AS Stroke,\n",
    "    CAST(HeartDiseaseorAttack AS DOUBLE) AS HeartDiseaseorAttack,\n",
    "    CAST(PhysActivity AS DOUBLE) AS PhysActivity,\n",
    "    CAST(Fruits AS DOUBLE) AS Fruits,\n",
    "    CAST(Veggies AS DOUBLE) AS Veggies,\n",
    "    CAST(HvyAlcoholConsump AS DOUBLE) AS HvyAlcoholConsump,\n",
    "    CAST(AnyHealthcare AS DOUBLE) AS AnyHealthcare,\n",
    "    CAST(NoDocbcCost AS DOUBLE) AS NoDocbcCost,\n",
    "    CAST(GenHlth AS DOUBLE) AS GenHlth,\n",
    "    CAST(MentHlth AS DOUBLE) AS MentHlth,\n",
    "    CAST(PhysHlth AS DOUBLE) AS PhysHlth,\n",
    "    CAST(DiffWalk AS DOUBLE) AS DiffWalk,\n",
    "    CAST(Sex AS DOUBLE) AS Sex,\n",
    "    CAST(Age AS DOUBLE) AS Age,\n",
    "    CAST(Education AS DOUBLE) AS Education,\n",
    "    CAST(Income AS DOUBLE) AS Income\n",
    "  FROM dbacademy.labuser9597408_1742230869.diabetes_binary\n",
    ")\n",
    "SELECT *, diabetes_apply(struct(*)) AS predictions , diabetes_proba(struct(*))[0] AS probability\n",
    "FROM inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b749d23-4209-4e70-84f1-f26cd6462d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This demo guided us through the process of building, evaluating, and interpreting a Decision Tree model for classification tasks. We started by preparing and splitting the dataset, then proceeded to train the model using a Feature Store table. We manually logged key parameters, metrics, and artifacts using MLflow tracking, facilitating comprehensive experiment tracking and reproducibility. We examined and logged the model's performance through a confusion matrix, analyzed feature importances, and visualized the resulting tree structure. By leveraging MLflow, we demonstrated effective model tracking and experimentation management, contributing to a more informed and accountable machine learning workflow.\n",
    "\n",
    "We also loaded the Unity Catalog registered model and created SQL UDF for both a binary and continuous inference based on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e729c83d-5d45-457b-864a-3793f19e10bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2257947333491615,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflowModelTrackingDeluxe",
   "widgets": {
    "catalog_name": {
     "currentValue": "dbacademy",
     "nuid": "2f9d2e7f-51da-4bb4-aec9-1819c41bf87d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbacademy",
      "label": "",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbacademy",
      "label": "",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "labuser9597408_1742230869",
     "nuid": "39bd642d-487d-439b-93a1-5d52d4fe5283",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
