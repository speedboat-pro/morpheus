{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d52a6f-d083-4ec3-b773-a7ebcca40174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.17.2 importlib-metadata==6.8.0 cloudpickle==2.0.0 zipp==3.16.2\n",
    "%pip install --ignore-installed Jinja2==3.1.2 markupsafe==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd844bc-8c16-4407-a1c5-5d7f6064e11d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ðŸš¨ Warning: Please don't run this notebook directly. This notebook must be used when creating the pipeline. Follow the instructions listed in the \"3.1.a - Pipeline Deployment\" notebook.**\n",
    "\n",
    "**ðŸš¨ Warning:** For this notebook to successfully run, you must have;\n",
    "* Trained and logged a model to the registry (e.g. `ml_model`)\n",
    "* Set the `catalog` and `schema` to point to your own. \n",
    "* Create pipeline parameters for input data path and model name (e.g. `mlpipeline.bronze_dataset_path`& `mlpipeline.model_name`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f9acaf2-7531-445e-8a84-755d7782d57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Inference Pipeline\n",
    "\n",
    " MLflow-trained models can be used in Delta Live Tables pipelines. MLflow models are treated as transformations in Databricks, meaning they act upon a Spark DataFrame input and return results as a Spark DataFrame. Because Delta Live Tables defines datasets against DataFrames, you can convert Apache Spark workloads that leverage MLflow to Delta Live Tables with just a few lines of code.\n",
    "\n",
    "If you already have a Python notebook calling an MLflow model, you can adapt the code to Delta Live Tables by using the `@dlt.table` decorator and ensuring functions are defined to return transformation results. For an introduction to Delta Live Tables syntax, see Tutorial: [Declare a data pipeline with Python in Delta Live Tables.](https://docs.databricks.com/en/delta-live-tables/tutorial-python.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11eb257-893a-4bac-a9a8-f01b5a2e198d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Pipeline configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db349ae-e4f4-4a91-8836-bbfae7b4d96d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_dataset_path = spark.conf.get(\"mlpipeline.bronze_dataset_path\")\n",
    "model_name = spark.conf.get(\"mlpipeline.model_name\")\n",
    "alias_name = spark.conf.get(\"mlpipeline.model_alias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a57a5c-469a-4cb0-b186-18e3f640fb58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bece6945-96d5-423c-9d67-e8039c46d3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri=f\"models:/{model_name}@{alias_name}\" \n",
    "loaded_model_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=model_uri, \n",
    "    result_type=\"string\",\n",
    "    env_manager = \"local\"\n",
    "    )\n",
    "\n",
    "primary_key = \"customerID\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ec38f08-5791-4e67-8dcc-c2d3a74a5475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "284dfb0a-75ad-4dc7-b32b-8108d5c55c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "407ed68c-521f-47e1-8a63-9b98032427fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "from pyspark.sql.functions import col, struct\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"raw_inputs_stream\",\n",
    "  comment=\"Raw inputs table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\"\n",
    "  } \n",
    ")\n",
    "def raw_inputs():\n",
    "  return spark.readStream.csv(bronze_dataset_path, schema= 'customerID STRING,gender STRING,SeniorCitizen DOUBLE,Partner STRING,Dependents STRING,tenure DOUBLE,PhoneService STRING,MultipleLines STRING,InternetService STRING,OnlineSecurity STRING,OnlineBackup STRING,DeviceProtection STRING,TechSupport STRING,StreamingTV STRING,StreamingMovies STRING,Contract STRING,PaperlessBilling STRING,PaymentMethod STRING,MonthlyCharges DOUBLE,TotalCharges STRING,Churn STRING', header=True, multiLine=True, escape='\"')\n",
    "  \n",
    "@dlt.table(\n",
    "  name=\"features_input_stream\",\n",
    "  comment=\"Features table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"silver\"\n",
    "  }\n",
    ")\n",
    "def features_input():\n",
    "  return (\n",
    "    dlt.read_stream(\"raw_inputs_stream\")\n",
    "    .select(primary_key, *features)\n",
    "    .withColumn(\"SeniorCitizen\",col(\"SeniorCitizen\").cast('double'))\n",
    "    .withColumn(\"tenure\",col(\"tenure\").cast('double'))\n",
    "    .withColumn(\"TotalCharges\",col(\"TotalCharges\").cast('double'))\n",
    "    .na.drop(how='any')\n",
    "  )\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"model_predictions_stream\",\n",
    "  comment=\"Inference table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"gold\"\n",
    "  }\n",
    ")\n",
    "def model_predictions():\n",
    "  return (\n",
    "    dlt.read_stream(\"features_input_stream\")\n",
    "    .withColumn(\"prediction\", loaded_model_udf(struct(features)))\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.1b Demo - Inference Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
