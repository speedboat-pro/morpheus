{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03de9bac-0add-4b51-94a5-e8de140661af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Benchmark / Source**                       | **Description & Domain Focus**                                                                                                                       | **Use-Case / Task Type**                                 | **URL**                                                                                                                                                                                                            |\n",
    "| -------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Golden Touchstone**                        | Bilingual (English + Chinese) **financial-domain** benchmark covering 8 core financial NLP tasks (information extraction, summarization, reasoning). | Finance, regulatory reporting, financial QA              | [arxiv.org/abs/2411.06272](https://arxiv.org/abs/2411.06272)                                                                                                                                                       |\n",
    "| **SOP-Bench**                                | Industrial-automation benchmark (1.8 k tasks, 10 domains) evaluating complex SOP (Standard Operating Procedures) for LLM agents.                     | Manufacturing, lab safety, process automation            | [arxiv.org/abs/2506.08119](https://arxiv.org/abs/2506.08119)                                                                                                                                                       |\n",
    "| **RAGBench**                                 | Retrieval-Augmented Generation benchmark (100 k examples) drawn from technical/industrial corpora.                                                   | Retrieval pipelines, domain Q&A, documentation reasoning | [arxiv.org/abs/2407.11005](https://arxiv.org/abs/2407.11005)                                                                                                                                                       |\n",
    "| **FailureSensorIQ**                          | Multi-choice QA dataset for “Sensor relationships and failure modes” in industrial assets (Industry 4.0).                                            | Predictive maintenance, asset diagnostics                | [arxiv.org/abs/2506.03278](https://arxiv.org/abs/2506.03278)                                                                                                                                                       |\n",
    "| **Eight Benchmark Suites for Cybersecurity** | Comparative review of 8 benchmark suites for **cybersecurity** (threat intel, phishing detection, etc.).                                             | Cybersecurity, compliance monitoring, SOC automation     | [infosecurityeurope.com blog](https://www.infosecurityeurope.com/en-gb/blog/future-thinking/top-8-llm-benchmarks-for-cybersecurity-practices.html)                                                                 |\n",
    "| **General LLM Benchmark Databases**          | Meta-collections (250 + benchmarks / datasets) with summaries and survey papers.                                                                     | Cross-industry reference set                             | [Evidently AI LLM Benchmarks](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets) • [A Survey on LLM Benchmarks (2025)](https://arxiv.org/html/2508.15361v1)                                           |\n",
    "| **InsQABench**                               | Chinese-language dataset for **insurance** sector; includes commonsense knowledge, structured & unstructured insurance documents.                    | Insurance QA / document reasoning                        | [arxiv.org/abs/2501.10943](https://arxiv.org/abs/2501.10943)                                                                                                                                                       |\n",
    "| **Multi-Turn Insurance Underwriting**        | Dialogue dataset built for **insurance underwriting** conversations (multi-turn QA with experts).                                                    | Insurance underwriting assistant evaluation              | [huggingface.co/datasets/snorkelai/Multi-Turn-Insurance-Underwriting](https://huggingface.co/datasets/snorkelai/Multi-Turn-Insurance-Underwriting)                                                                 |\n",
    "| **Insurance LLM Framework**                  | Open-source framework for prompt engineering + evaluation of LLMs in insurance tasks.                                                                | Evaluation framework, prompt design tests                | [github.com/ozturkoktay/insurance-llm-framework](https://github.com/ozturkoktay/insurance-llm-framework)                                                                                                           |\n",
    "| **State of AI in Insurance (Vol. II)**       | Research report comparing LLM performance across insurance use-cases (claims, fraud detection, underwriting).                                        | Benchmark report, enterprise meta-evaluation             | [shift-technology.com/resources/research/the-state-of-ai-in-insurance-a-comparison-of-llms-vol.-ii](https://www.shift-technology.com/resources/research/the-state-of-ai-in-insurance-a-comparison-of-llms-vol.-ii) |\n",
    "| **LXT AI LLM Benchmarks Blog**               | Overview and classification of LLM benchmarks by industry and task type.                                                                             | General survey / reference list                          | [lxt.ai/blog/llm-benchmarks](https://www.lxt.ai/blog/llm-benchmarks/)                                                                                                                                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c959b6-2ab6-4020-bfd4-8d96b5328c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM Benchmark References",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
