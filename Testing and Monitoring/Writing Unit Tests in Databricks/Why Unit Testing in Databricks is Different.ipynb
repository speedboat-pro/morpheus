{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814ec7a5-8b76-404c-a69d-c2259c79d920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Adapted from the [Original article](https://community.databricks.com/t5/technical-blog/writing-unit-tests-for-pyspark-in-databricks-approaches-and-best/ba-p/122398)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae9d4a64-c495-4663-a271-c78fa52ad4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction \n",
    "Unit testing is a vital part of software development, ensuring that individual components of your code function as intended. However, when working with PySpark in Databricks, many teams find themselves skipping or minimising unit tests due to a range of unique challenges. Spark's distributed nature, the interactive workflow of Databricks notebooks, and the integration with Databricks-specific tools can make traditional testing approaches feel cumbersome or out of place.\n",
    "\n",
    "Common issues include difficulty creating isolated test environments, challenges in mocking Spark dataframes and Databricks utilities, and uncertainty about how to automate tests within the Databricks ecosystem. As a result, teams may rely heavily on manual testing or integration tests, which are slower, harder to maintain, and more likely to let bugs slip through.\n",
    "\n",
    "Yet, skipping unit tests comes at a cost: without them, bugs in data transformations or business logic can make their way into production, leading to costly data errors, pipeline failures, or customer impact. Well-designed unit tests catch issues early, accelerate development, and provide confidence during refactoring or scaling.\n",
    "\n",
    "This blog outlines practical strategies for writing unit tests for PySpark applications in Databricks. Weâ€™ll discuss common pitfalls, share actionable tips, and highlight best practices to help you build a robust testing culture-ensuring your Spark code is reliable, maintainable, and production-ready, even within the unique context of Databricks.\n",
    "\n",
    "Why Unit Testing in Databricks is Different\n",
    "Databricks provides a managed platform for big data processing and machine learning that leverages Apache Spark. However, testing PySpark code within Databricks comes with unique challenges:\n",
    "\n",
    "Runtime-Specific Libraries: Code often relies on Databricks utilities like dbutils, which are unavailable outside the Databricks environment.\n",
    "Global SparkSession: The SparkSession provided by Databricks is automatically initialised and may not be accessible outside its runtime.\n",
    "Notebook-Based Workflows: Many workflows are written in notebooks, which can complicate modular testing.\n",
    "Despite these challenges, unit testing remains essential to ensure code reliability and maintainability.\n",
    "\n",
    "Strategies for Writing Unit Tests in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d7c5693-8f3a-43e9-8bbd-3d8aa554b3d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Refactor Code for Better Testability\n",
    "To make your PySpark code easier to test:\n",
    "\n",
    "Extract Transformation Logic: Move data processing logic into standalone Python functions or modules.\n",
    "Minimise Direct Dependencies on dbutils: Use dependency injection or mock objects to replace dbutils calls during testing.\n",
    "Control Notebook Execution: Wrap main execution logic in if __name__ == \"__main__\" blocks to prevent it from running during imports.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5daa95d-a0c9-43ec-9122-47b83957fc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def process_data(df):\n",
    "    return df.select(\"name\", \"birthDate\").filter(F.col(\"dob\") >= F.lit(\"2000-01-01\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Main notebook logic\n",
    "    uc_volume_path = \"volume://my_catalog.my_schema.my_volume/my_data\"\n",
    "    dbutils.fs.mkdirs(uc_volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f841ea9a-871e-4e91-a620-d0d0bbd309e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By isolating the process_data function, you can test it independently without invoking the notebook's runtime-specific code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b89099-106f-4935-9e18-fdb61b79bdfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Use Pytest with Fixtures\n",
    "Pytest offers a flexible framework for writing tests. It allows you to use fixtures to set up reusable resources like a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2932a2-94a0-4c81-b543-d55388db4453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from databricks_notebook import process_data\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    return SparkSession.builder.master(\"local[*]\").appName(\"PyTest\").getOrCreate()\n",
    "\n",
    "def test_process_data(spark_session):\n",
    "    data = [(\"Alpha\", \"2000-01-01\"), (\"Beta\", \"1980-05-12\")]\n",
    "    schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"birthDate\", StringType(), True)\n",
    "    ])\n",
    "    df = spark_session.createDataFrame(data, schema)\n",
    "    result_df = process_data(df)\n",
    "\n",
    "    assert result_df.count() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a66376f8-ec89-4b07-a0cb-b16e7a53c4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This approach ensures modularity and allows you to reuse the same SparkSession across multiple test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f6792a-888e-4590-bf4e-98430be68a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Organise Code Using Databricks Repos\n",
    "Databricks Repos enable better organisation of your code by storing PySpark functions in .py files within a repository. Using Databricks Asset Bundles (DABs), you can define and deploy resources programmatically for CI/CD workflows with simple YAML configurations. Tests can be written in separate files (e.g., test_functions.py) using frameworks like pytest and executed directly in Databricks Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07debc50-b319-4010-a24e-5e75d15ccd75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Steps to Run Tests in Databricks Notebooks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "197753fc-2875-4eda-a55d-b03caf743a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install pytest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c21bb0e1-33cc-4ade-9309-c11b36a4ddf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7e7a430-741f-437d-aa80-01715b0710a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d115059-5531-4286-8d8d-79a62475ce4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "retcode = pytest.main([\".\", \"-v\", \"-p\", \"no:cacheprovider\"])\n",
    "\n",
    "assert retcode == 0, \"Some tests failed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe06442e-99e2-4ea4-bc9e-714703481ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This setup integrates seamlessly with Databricks Repos, DABs, version control systems, and CI/CD pipelines, enhancing code modularity and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d794d7f9-bc2a-48de-b653-bf8de2196519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Validate DataFrames Using Equality Checks\n",
    "Starting from Apache Spark 3.5 (and Databricks Runtime 14.2), built-in methods like assertDataFrameEqual and assertSchemaEqual simplify DataFrame validation.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbcb2bb9-0aa4-4778-a9b1-b273cae23f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "def test_dataframe_equality(spark_session):\n",
    "    df1 = spark_session.createDataFrame([(\"Alpha\", 20)], [\"name\", \"age\"])\n",
    "    df2 = spark_session.createDataFrame([(\"Alpha\", 20)], [\"name\", \"age\"])\n",
    "\n",
    "    assertDataFrameEqual(df1, df2)  # Passes if both DataFrames are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c5169c4-a3f1-47dc-a136-b9d457880905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These methods are particularly useful for verifying complex transformations or schema changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a74bac78-ce47-4fc6-9c4b-95cdc36a7cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Mocking Databricks-Specific Libraries\n",
    "Databricks utilities like dbutils can be mocked using Python's unittest.mock module to simulate their behaviour during testing.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c2488ce-1a24-45ec-aa40-f667b107f50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock\n",
    "\n",
    "def test_dbutils_interaction():\n",
    "    mock_dbutils = MagicMock()\n",
    "    mock_dbutils.fs.mkdirs.return_value = None\n",
    "\n",
    "   # Simulate function call\n",
    "    uc_volume_path = \"volume://my_catalog.my_schema.my_volume/my_data\"\n",
    "    mock_dbutils.fs.mkdirs(uc_volume_path)\n",
    "\n",
    "    mock_dbutils.fs.mkdirs.assert_called_once_with(uc_volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60415baa-3a3d-4983-8ab4-bc6d76b7be27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Other References\n",
    "[Simplify PySpark testing with DataFrame equality functions](https://www.databricks.com/blog/simplify-pyspark-testing-dataframe-equality-functions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Why Unit Testing in Databricks is Different",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
