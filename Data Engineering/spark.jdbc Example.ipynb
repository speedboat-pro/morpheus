{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e32a9418-e549-48d4-8428-489e55e00215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cfg = {\n",
    "    \"config\":\n",
    "        {\n",
    "            # ALL FIELDS ARE MANDATORY IN THIS DICTIONARY\n",
    "             \"jdbcHostname\":\"10.10.9.4\" # Source Server Address\n",
    "            ,\"jdbcPort\":1433    # Source Server Port\n",
    "            ,\"jdbcSourceDatabase\":\"SourceDB\" # Source Server Database \n",
    "            ,\"jdbcUsername\":\"USerName\" # Source Server user\n",
    "            ,\"jdbcPassword\":\"password\" # Source Server password\n",
    "            ,\"jdbcDriver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\" # Source Server JDBC Driver\n",
    "            ##\n",
    "            #,\"jdbcDriver\":\"oracle.jdbc.driver.OracleDriver\" # Oracle JDBC Driver\n",
    "            #,\"jdbcDriver\":\"com.ibm.db2.jcc.DB2Driver\" # DB2\n",
    "            #,\"jdbcDriver\":\"org.mariadb.jdbc.Driver\" # MariaDB/MySQL\n",
    "            #,\"jdbcDriver\":\"org.postgresql.Driver\" # PostgreSQL\n",
    "            ,\"jdbcSourceSchema\":\"triyam\" # Source Database Schema\n",
    "            #,\"targetDatabase\":\"koantek_parallel_load\" Target databse for Unity Catalog implementation\n",
    "            ,\"targetSchema\":\"koantek_triyam_test\" # Target Server Schema\n",
    "            ,\"parallelCores\":8 # Number of vCPUs available in cluster\n",
    "        }\n",
    ",    \"load\":\n",
    "        [\n",
    "            {\n",
    "                # ONLY table and keyField FIELDS ARE MANDATORY IN THIS DICTIONARY\n",
    "                \"table\":\"order_entry_fields\" # source table name\n",
    "                ,\"keyField\":\"OE_FIELD_ID\" # source keyfield column name (int, bigint or float)\n",
    "             }\n",
    "            ,\n",
    "            {\n",
    "                \"table\":\"oe_format_fields\"\n",
    "                ,\"keyField\":\"OE_FORMAT_ID\"\n",
    "                # OPTIONAL FIELDS IN THIS DICTIONARY - will increase load time significantly\n",
    "                # NOTES\n",
    "                # Clustering on a per table will rewrite table after parallel import\n",
    "                # Clustering on a per table basis will disable Z-Order and Partitioning\n",
    "                ,\"partition\":[\"ACCEPT_FLAG\"] # partitioning column\n",
    "                ,\"zorder\":[\"FIELD_SEQ\"] # Z-Order column \n",
    "                ,\"cluster\":[\"EPILOG_METHOD\"] # Liquid Clustering Columns\n",
    "             }\n",
    "        ]    \n",
    "}\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f20267a-2392-4a5d-a14e-f425805d09d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def ParallelJDBCLoad():\n",
    "    json_cfg = dbutils.widgets.get(\"configuration_file\") #URI to cfg JSON file\n",
    "    with open(json_cfg, 'r') as openfile:\n",
    "        json_cfg = ast.literal_eval(openfile.read())        \n",
    "    dbcfg = json_cfg[\"config\"]\n",
    "    jdbcHostname = dbcfg[\"jdbcHostname\"]\n",
    "    jdbcPort = dbcfg[\"jdbcPort\"]\n",
    "    jdbcDatabase = dbcfg[\"jdbcSourceDatabase\"]\n",
    "    jdbcUsername = dbcfg[\"jdbcUsername\"]\n",
    "    jdbcPassword = dbcfg[\"jdbcPassword\"]\n",
    "    jdbcDriver = dbcfg[\"jdbcDriver\"]\n",
    "    jdbcSchema = dbcfg[\"jdbcSourceSchema\"]\n",
    "    targetSchema = dbcfg[\"targetSchema\"]\n",
    "    numPartitions = dbcfg[\"parallelCores\"]\n",
    "    jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};    user={jdbcUsername};password={jdbcPassword};TrustServerCertificate=True\" \n",
    "\n",
    "    for k in json_cfg[\"load\"]:\n",
    "        tbl = k[\"table\"]\n",
    "        keyField = k[\"keyField\"]\n",
    "        partition = k.get(\"partition\") if k.get(\"partition\") else None\n",
    "        z_order = \",\".join( k.get(\"zorder\")) if k.get(\"zorder\") else None\n",
    "        cluster = \",\".join( k.get(\"cluster\")) if k.get(\"cluster\") else None\n",
    "        print(f\"{jdbcSchema}.{tbl}\")\n",
    "        tblconfig = (\n",
    "            spark.read.jdbc(url=jdbcUrl, table=f\"{jdbcSchema}.{tbl}\")\n",
    "                .select(\n",
    "                    F.min(F.col(keyField)).alias(\"min_value\"),\n",
    "                    F.max(col(keyField)).alias(\"max_value\"),\n",
    "                    F.count(\"*\").alias(\"row_count\"))  )  \n",
    "        # Extract the values from the result\n",
    "        lower = int(tblconfig.collect()[0][\"min_value\"])\n",
    "        #print(f\"min_value {lower}\")\n",
    "        upper = int(tblconfig.collect()[0][\"max_value\"])\n",
    "        #print(f\"max_value {upper}\")\n",
    "        row_count = int(tblconfig.collect()[0][\"row_count\"])\n",
    "        #print(f\"row_count {row_count}\")\n",
    "        print(str(datetime.now()) + \"> Starting to import \" + tbl)\n",
    "        dft = spark.read.jdbc(url=jdbcUrl, table=f\"{jdbcSchema}.{tbl}\"\n",
    "                            , column=keyField, lowerBound=lower, upperBound=upper, numPartitions=numPartitions)\n",
    "        tries = 3\n",
    "\n",
    "        for i in range(tries):\n",
    "            try:\n",
    "                start_time =    str(datetime.now())\n",
    "                #spark.catalog.setCurrentDatabase(targetDatabase)\n",
    "                if(partition and cluster == None):\n",
    "                    print(str(start_time) + f\"> Partitioning {targetSchema}.{tbl} by {partition}\")\n",
    "                    dft.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(partition).saveAsTable(f\"{targetSchema}.{tbl}\")\n",
    "                else:\n",
    "                    dft.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{targetSchema}.{tbl}\")\n",
    "            except Exception as e:\n",
    "                if i < tries - 1: \n",
    "                    print(str(datetime.now()) + f\"> Exception attempting to sync for {targetSchema}.{tbl} - retrying in 15 seconds. Exception: {e}\")\n",
    "                    time.sleep(15)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(str(datetime.now()) + f\"> MAX number of retries reached, Exception: {e}\")\n",
    "                raise\n",
    "            break\n",
    "        if(cluster):\n",
    "            print(str(datetime.now()) + f\"> Clustering {targetSchema}.{tbl} by {cluster}\")\n",
    "            spark.sql(f\"CREATE TABLE {targetSchema}.{tbl}_temp CLUSTER BY ({cluster}) AS SELECT * FROM {targetSchema}.{tbl}\")\n",
    "            spark.sql(f\"DROP TABLE {targetSchema}.{tbl}\")\n",
    "            spark.sql(f\"ALTER TABLE {targetSchema}.{tbl}_temp RENAME TO {targetSchema}.{tbl}\")            \n",
    "        if(z_order and cluster == None):\n",
    "            print(str(datetime.now()) + f\"> Optimizing {targetSchema}.{tbl} by {z_order}\")\n",
    "            spark.sql(f\"OPTIMIZE {targetSchema}.{tbl} ZORDER BY ({z_order})\")\n",
    "        end_time = str(datetime.now())\n",
    "        end_rowcount = spark.read.table(f\"{targetSchema}.{tbl}\").select(\n",
    "            F.count(\"*\").alias(\"row_count\")).collect()[0].asDict()[\"row_count\"]\n",
    "     \n",
    "        print(end_time + f\"> Import completed for {targetSchema}.{tbl}\")\n",
    "        new_row = Row(\n",
    "        f\"{targetSchema}.{tbl}\" # table_name\tSTRINg\n",
    "        , start_time# ,load_begin\tTIMESTAMP\n",
    "        , end_time # ,load_end\tTIMESTAMP\n",
    "        , row_count # ,begin_rowcount INT\n",
    "        , end_rowcount # ,end_rowcount INT\n",
    "        , end_rowcount/(\n",
    "            (datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S.%f\") - datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S.%f\")).total_seconds()\n",
    "            )# ,rows_per_minute FLOAT\n",
    "        # Insert the row into the table\n",
    "        )\n",
    "\n",
    "        # Convert the row to a DataFrame\n",
    "        new_row_df = spark.createDataFrame([new_row])\n",
    "        new_row_df.write.insertInto(f\"{targetSchema}.load_metrics\", overwrite=False)\"\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark.jdbc Example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
