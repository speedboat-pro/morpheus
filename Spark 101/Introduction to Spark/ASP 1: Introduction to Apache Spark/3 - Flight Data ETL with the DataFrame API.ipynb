{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b4d68c-ab8b-4103-80f7-af3af0abe76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0ce5bf5-b463-4a83-9c76-7a78063b9d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 - Flight Data ETL with the DataFrame API\n",
    "\n",
    "This demonstration will walk through common ETL operations using the Flights dataset. We'll cover data loading, cleaning, transformation, and analysis using the DataFrame API.\n",
    "\n",
    "### Objectives\n",
    "- Implement common ETL operations using Spark DataFrames\n",
    "- Handle data cleaning and type conversion\n",
    "- Create derived features through transformations\n",
    "- Use different column reference methods\n",
    "- Work with User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb15c7f0-1cdf-4dee-a5df-eda23118bbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "673a4ea9-f396-4df0-9e00-cfa6b395ce00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96b81ac-fd86-41db-8d5f-3ed06f2b5be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5069e34e-0724-49c2-b214-5225b6b13a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View your default catalog and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800bacb7-399f-4261-9cc3-89265f13f4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05ddf89a-e1e2-4c49-a154-7752e366720c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Flight Data Processing Requirements\n",
    "\n",
    "#### Source Data\n",
    "Dataset Location: `dbacademy_airline.v01.flights_small`(flight information dataset)\n",
    "\n",
    "#### Target\n",
    "Table name: flight_data\n",
    "\n",
    "Schema:\n",
    "\n",
    "| Column Name | Data Type | Description |\n",
    "|-------------|-----------|-------------|\n",
    "| FlightDateTime | datetime | Datetime of the flight (derived from the Year, Month, DayofMonth, DepTime fields in the source data) |\n",
    "| FlightNum | integer | Flight number |\n",
    "| ElapsedTimeDiff | integer | Difference between scheduled elapsed time and actual elapsed time for the flight, derived from the ActualElapsedTime and CRSElapsedTime fields in the source data |\n",
    "| ArrDelayCategory | string | Categories include \"On Time\", \"Slight Delay\", \"Moderate Delay\" and \"Severe Delay\" based upon the value of the ArrDelay in the source data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa42f756-7f05-4231-a869-339c78f66e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Data Loading and Inspection\n",
    "\n",
    "First, let's load and inspect the flight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c34f9d-b657-482c-abc0-eb2348a8ef35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_df = spark.sql(f\"\"\"\n",
    "                       SELECT * FROM \n",
    "                       dbacademy_airline.v01.flights_small\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981832ca-2625-4a68-87eb-0e5cc071f5f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE DETAIL dbacademy_airline.v01.flights_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a294be55-9e4a-464f-83ee-c64064337bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the flights data\n",
    "flights_df2 = spark.read.table(\"dbacademy_airline.v01.flights_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94b1d36-466c-43d8-9302-72c0ae9506bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing import assertDataFrameEqual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58b4da0-c615-4183-9787-6cb0eef3150b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assertDataFrameEqual(flights_df, flights_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711b1f77-9389-49b0-92f7-eb79177b6795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.testing import assertSchemaEqual\n",
    "\n",
    "assertSchemaEqual(flights_df.schema, flights_df2.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3077634-8cd9-42bd-b315-ec3ccb789f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the schema\n",
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbefa93-ba2e-481c-ad21-06c9835f080b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visually inspect a subset of the data\n",
    "display(flights_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f342eb1-ba87-460b-a981-46748d135be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's remove columns we dont need, remember \"filter early, filter often\"\n",
    "flights_required_cols_df = flights_df.select(\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"DayofMonth\",\n",
    "    \"DepTime\",\n",
    "    \"FlightNum\",\n",
    "    \"ActualElapsedTime\",\n",
    "    \"CRSElapsedTime\",\n",
    "    \"ArrDelay\")\n",
    "\n",
    "# Alternatively we could have used the drop() method to remove the columns we didnt want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f15567c-8c26-46b8-81a6-20b01c7b341f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a count of the source data records\n",
    "initial_count = flights_required_cols_df.count()\n",
    "\n",
    "print(f\"Source data has {initial_count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e373d3-a4c8-4aa8-b601-31859db59e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's examine the data for invalid values, these can include nulls or invalid values for string columns \"ArrDelay\", \"ActualElapsedTime\", \"DepTime\" which we intend on performing mathematical operations on, we can use the Spark SQL COUNT_IF function to perform the analysis\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table with cast columns\n",
    "flights_required_cols_df \\\n",
    "    .selectExpr(\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"DayofMonth\",\n",
    "        \"CAST(DepTime AS INT) AS DepTime\",\n",
    "        \"FlightNum\",\n",
    "        \"CAST(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "        \"CRSElapsedTime\",\n",
    "        \"CAST(ArrDelay AS INT) AS ArrDelay\"\n",
    "    ) \\\n",
    "    .createOrReplaceTempView(\"flights_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc73bb89-1a0e-4487-809a-30e945624458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Spark SQL to count null values\n",
    "invalid_counts_sql = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT_IF(Year IS NULL) AS Null_Year_Count,\n",
    "    COUNT_IF(Month IS NULL) AS Null_Month_Count,\n",
    "    COUNT_IF(DayofMonth IS NULL) AS Null_DayOfMonth_Count,\n",
    "    COUNT_IF(DepTime IS NULL) AS Null_DepTime_Count,\n",
    "    COUNT_IF(FlightNum IS NULL) AS Null_FlightNum_Count,\n",
    "    COUNT_IF(ActualElapsedTime IS NULL) AS Null_ActualElapsedTime_Count,\n",
    "    COUNT_IF(CRSElapsedTime IS NULL) AS Null_CRSElapsedTime_Count,\n",
    "    COUNT_IF(ArrDelay IS NULL) AS Null_ArrDelay_Count\n",
    "FROM flights_temp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f4ee24-9482-4067-953b-3dc389b35d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(invalid_counts_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0842f17d-25e3-472e-810f-b7fe9a9ec240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Comparing Spark SQL to DataFrame API Operations\n",
    "Spark SQL DataFrame queries and their equivalent operations in the DataFrame API are evaluated to the same physical plans, let's prove this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1f7883-fd1d-4a80-aee0-2ab3c1555ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(invalid_counts_df.printSchema())\n",
    "print(invalid_counts_sql.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca93a549-47c5-47b6-82ee-c825278c6c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_counts_df.schema == invalid_counts_sql.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02221b3-4b13-40f6-ac57-e4dc7ac8f5c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc19abcf-2cec-498a-9bd9-e668631c13ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this is the equivalent of the preceding Spark SQL query using the DataFrame API\n",
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# Make sure to work with the same temporary view that the SQL is using\n",
    "flights_temp_df = spark.table(\"flights_temp\")\n",
    "\n",
    "'''\n",
    "SELECT \n",
    "    COUNT_IF(Year IS NULL) AS Null_Year_Count,\n",
    "    COUNT_IF(Month IS NULL) AS Null_Month_Count,\n",
    "    COUNT_IF(DayofMonth IS NULL) AS Null_DayOfMonth_Count,\n",
    "    COUNT_IF(DepTime IS NULL) AS Null_DepTime_Count,\n",
    "    COUNT_IF(FlightNum IS NULL) AS Null_FlightNum_Count,\n",
    "    COUNT_IF(ActualElapsedTime IS NULL) AS Null_ActualElapsedTime_Count,\n",
    "    COUNT_IF(CRSElapsedTime IS NULL) AS Null_CRSElapsedTime_Count,\n",
    "    COUNT_IF(ArrDelay IS NULL) AS Null_ArrDelay_Count\n",
    "FROM flights_temp\n",
    "'''\n",
    "# Use DataFrame API to count null values\n",
    "invalid_counts_df = flights_temp_df.select(\n",
    "    sum(when(col(\"Year\").isNull(), 1).otherwise(0)).alias(\"Null_Year_Count\"),\n",
    "    sum(when(col(\"Month\").isNull(), 1).otherwise(0)).alias(\"Null_Month_Count\"),\n",
    "    sum(when(col(\"DayofMonth\").isNull(), 1).otherwise(0)).alias(\"Null_DayOfMonth_Count\"),\n",
    "    sum(when(col(\"DepTime\").isNull(), 1).otherwise(0)).alias(\"Null_DepTime_Count\"),\n",
    "    sum(when(col(\"FlightNum\").isNull(), 1).otherwise(0)).alias(\"Null_FlightNum_Count\"),\n",
    "    sum(when(col(\"ActualElapsedTime\").isNull(), 1).otherwise(0)).alias(\"Null_ActualElapsedTime_Count\"),\n",
    "    sum(when(col(\"CRSElapsedTime\").isNull(), 1).otherwise(0)).alias(\"Null_CRSElapsedTime_Count\"),\n",
    "    sum(when(col(\"ArrDelay\").isNull(), 1).otherwise(0)).alias(\"Null_ArrDelay_Count\")\n",
    ")\n",
    "\n",
    "display(invalid_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d29303-45b7-4f82-8de2-96857dc65b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the explain plans for the SQL and DF versions of our query\n",
    "sql_plan = invalid_counts_sql.explain() #Getting SQL Plan Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c7cf6f-0c11-4663-8a86-bfb781dead28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_plan = invalid_counts_df.explain() # Getting DF Plan Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548114d2-d2dc-4c72-9ce9-410533a19d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show that the two approaches evaluate to the same physical plan\n",
    "sql_plan == df_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2d19b5e-7d51-4f3d-bde0-8b4a9c31b84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Using the Databricks AI Assistant\n",
    "The Databricks AI Assistant feature can be used to generate code or to visualize metrics from DataFrames, from the code cell below click on the __generate__ link and enter:\n",
    "\n",
    "```generate a bar chart showing nulls for each column in the flights_temp_df dataframe```\n",
    "\n",
    "**NOTE:** Click on AI assistance toggle button and Enter the given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6294896-00f5-4b61-b70a-8b803f75897c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf23c8c8-ba08-484c-8db1-adde0aae87ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Add your AI generated code from above cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c414d81b-2754-4297-beaa-e461aeae8527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Data Cleaning\n",
    "\n",
    "The flights data contains some invalid and missing values, lets find them and clean them (in this case we will drop them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d577dc4c-a1c3-41cb-9be6-2609f3be83fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To drop rows where any specified columns are null, we can use the na.drop DataFrame method\n",
    "non_null_flights_df = flights_required_cols_df.na.drop(\n",
    "    how='any',\n",
    "    subset=['CRSElapsedTime']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4200a2e-1fd5-4d26-883b-a56cf8813e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Let's remove rows with invalid values for \"ArrDelay\", \"ActualElapsedTime\" and \"DepTime\" columns\n",
    "flights_with_valid_data_df = non_null_flights_df.filter(\n",
    "    col(\"ArrDelay\").cast(\"integer\").isNotNull() & \n",
    "    col(\"ActualElapsedTime\").cast(\"integer\").isNotNull() &\n",
    "    col(\"DepTime\").cast(\"integer\").isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f776874b-242b-41cb-ba62-92b9eb2e7aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now that we know \"ArrDelay\" and \"ActualElapsedTime\" contain integer values only, lets cast them from strings to integers (replacing the existing columns)\n",
    "clean_flights_df = flights_with_valid_data_df \\\n",
    "    .withColumn(\"ArrDelay\", col(\"ArrDelay\").cast(\"integer\")) \\\n",
    "    .withColumn(\"ActualElapsedTime\", col(\"ActualElapsedTime\").cast(\"integer\"))\n",
    "\n",
    "clean_flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ae06850-6894-4aa5-bb59-d5be6345ea53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Data Enrichment\n",
    "\n",
    "Now let's create a useful derived column to categorize delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9aa302a-a7a6-40b6-b0fc-df8eb659299f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's start by deriving the \"FlightDateTime\" column from the \"Year\", \"Month\", \"DayofMonth\", \"DepTime\" columns, then drop the constituent columns\n",
    "from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substr, lit\n",
    "\n",
    "flights_with_datetime_df = clean_flights_df.withColumn(\n",
    "    \"FlightDateTime\",\n",
    "    make_timestamp_ntz(\n",
    "        col(\"Year\"),\n",
    "        col(\"Month\"),\n",
    "        col(\"DayofMonth\"),\n",
    "        substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(1), lit(2)).cast(\"integer\"),\n",
    "        substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(3), lit(2)).cast(\"integer\"),\n",
    "        lit(0)\n",
    "    )\n",
    ").drop(\"Year\", \"Month\", \"DayofMonth\", \"DepTime\")\n",
    "\n",
    "# Show the result\n",
    "display(flights_with_datetime_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b875f200-2d76-4348-a1ea-b6dff120cf25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets derive the \"ElapsedTimeDiff\" column from the \"ActualElapsedTime\" and \"CRSElapsedTime\" columns\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "flights_with_elapsed_time_diff_df = flights_with_datetime_df.withColumn(\n",
    "    \"ElapsedTimeDiff\", col(\"ActualElapsedTime\") - col(\"CRSElapsedTime\")\n",
    "    ).drop(\"ActualElapsedTime\", \"CRSElapsedTime\")\n",
    "\n",
    "display(flights_with_elapsed_time_diff_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc041a7f-63a0-4044-a4fa-5f3bf8faa249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now lets categorize the \"ArrDelay\" column into categories: \"On Time\", \"Slight Delay\", \"Moderate Delay\", \"Severe Delay\"\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "enriched_flights_df = flights_with_elapsed_time_diff_df \\\n",
    "    .withColumn(\"delay_category\", when(col(\"ArrDelay\") <= 0, \"On Time\")\n",
    "        .when(col(\"ArrDelay\") <= 15, \"Slight Delay\")\n",
    "        .when(col(\"ArrDelay\") <= 60, \"Moderate Delay\")\n",
    "        .otherwise(\"Severe Delay\")) \\\n",
    "       .drop(\"ArrDelay\")\n",
    "    \n",
    "display(enriched_flights_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a30c440a-33de-4b72-b845-3e6b42b918c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Analyze Delays\n",
    "\n",
    "Let's analyze our delay categories using various column referencing approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5675029a-6e63-4f12-a944-bfe673320cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Direct reference to list 100 random records\n",
    "display(enriched_flights_df.select(\"FlightNum\", \"delay_category\").limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08aec24e-5e0f-45a6-b7d2-91ade6aed370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBDb2x1bW4gb2JqZWN0CmRpc3BsYXkoZW5yaWNoZWRfZmxpZ2h0c19kZi5zZWxlY3QoY29sKCJGbGlnaHROdW0iKS5hbGlhcygiY2Fycmllcl9jb2RlIiksIGNvbCgiZGVsYXlfY2F0ZWdvcnkiKSkubGltaXQoMTAwKSk=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView30833f6\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView30833f6\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView30833f6\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView30833f6) SELECT `delay_category`,COUNT(*) `column_e86f7f141088` FROM q GROUP BY `delay_category`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView30833f6\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "delay_category",
             "id": "column_e86f7f141087"
            },
            "y": [
             {
              "column": "*",
              "id": "column_e86f7f141088",
              "transform": "COUNT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_e86f7f141088": {
             "name": "carrier_code",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "f5dd7b13-5c66-458c-9d95-ac87cf6309ad",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 36.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "delay_category",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "delay_category",
           "type": "column"
          },
          {
           "alias": "column_e86f7f141088",
           "args": [
            {
             "column": "*",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Column object\n",
    "display(enriched_flights_df.select(col(\"FlightNum\").alias(\"carrier_code\"), col(\"delay_category\")).limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92836e2c-81a1-47a6-9d13-d88f39b482f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBTdHJpbmcgZXhwcmVzc2lvbnMKZGlzcGxheShlbnJpY2hlZF9mbGlnaHRzX2RmLnNlbGVjdEV4cHIoIkZsaWdodE51bSIsICJFbGFwc2VkVGltZURpZmYiLCAiRWxhcHNlZFRpbWVEaWZmID4gMCBhcyBMb25nZXJUaGFuU2NoZWR1bGVkIikp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView532cb94\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView532cb94\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView532cb94\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView532cb94) SELECT `FlightNum`,`ElapsedTimeDiff`,`LongerThanScheduled` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView532cb94\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "LongerThanScheduled",
             "id": "column_e86f7f141095"
            },
            "x": {
             "column": "FlightNum",
             "id": "column_e86f7f141093"
            },
            "y": [
             {
              "column": "ElapsedTimeDiff",
              "id": "column_e86f7f141094"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_e86f7f141094": {
             "name": "ElapsedTimeDiff",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "80bb18f1-bcb8-49e4-a5c0-5e26dfe7a4c2",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 37.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "FlightNum",
           "type": "column"
          },
          {
           "column": "ElapsedTimeDiff",
           "type": "column"
          },
          {
           "column": "LongerThanScheduled",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# String expressions\n",
    "display(enriched_flights_df.selectExpr(\"FlightNum\", \"ElapsedTimeDiff\", \"ElapsedTimeDiff > 0 as LongerThanScheduled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "987bd970-8b69-4095-ba1c-efd152fb7445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Working with UDFs\n",
    "\n",
    "Let's use a vectorized UDF to calculate the z-score (standard deviations from the mean) for delays for each flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ca094a-beb1-4a68-aa32-f6b1b5600436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IHBhbmRhc191ZGYKCiMgUGFuZGFzIFVERiAodmVjdG9yaXplZCkKQHBhbmRhc191ZGYoImRvdWJsZSIpCmRlZiBub3JtYWxpemVkX2RpZmYoZGlmZl9zZXJpZXMpOgogICAgcmV0dXJuIChkaWZmX3NlcmllcyAtIGRpZmZfc2VyaWVzLm1lYW4oKSkgLyBkaWZmX3Nlcmllcy5zdGQoKQoKIyBBcHBseSBib3RoIFVERnMKdWRmX2V4YW1wbGUgPSBlbnJpY2hlZF9mbGlnaHRzX2RmIFwKICAgIC53aXRoQ29sdW1uKCJkaWZmX25vcm1hbGl6ZWQiLCBub3JtYWxpemVkX2RpZmYoIkVsYXBzZWRUaW1lRGlmZiIpKQoKZGlzcGxheSh1ZGZfZXhhbXBsZSkKCiMgTm90ZTogSW4gcHJhY3RpY2UsIHByZWZlciBidWlsdC1pbiBmdW5jdGlvbnMgb3ZlciBVREZzIHdoZW4gcG9zc2libGU=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView0d3acde\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView0d3acde\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView0d3acde\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView0d3acde) ,min_max AS (SELECT `diff_normalized`,(SELECT MAX(`diff_normalized`) FROM q) `target_column_max`,(SELECT MIN(`diff_normalized`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `diff_normalized`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 100 `step` FROM min_max) SELECT IF(ISNULL(`diff_normalized`),NULL,LEAST(WIDTH_BUCKET(`diff_normalized`,`min_value`,`max_value`,100),100)) `diff_normalized_BIN`,FIRST(`min_value` + ((IF(ISNULL(`diff_normalized`),NULL,LEAST(WIDTH_BUCKET(`diff_normalized`,`min_value`,`max_value`,100),100)) - 1) * `step`)) `diff_normalized_BIN_LOWER_BOUND`,FIRST(`step`) `diff_normalized_BIN_STEP`,COUNT(`diff_normalized`) `COUNT` FROM histogram_meta GROUP BY `diff_normalized_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView0d3acde\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "diff_normalized",
             "id": "column_e86f7f141099"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 100,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1757522442462,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         65
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "d752db72-c014-452c-b6cf-8f266f875692",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 39.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1757522429979,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "diff_normalized_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "diff_normalized_BIN",
           "args": [
            {
             "column": "diff_normalized",
             "type": "column"
            },
            {
             "number": 100,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "diff_normalized_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "diff_normalized",
             "type": "column"
            },
            {
             "number": 100,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "diff_normalized_BIN_STEP",
           "args": [
            {
             "column": "diff_normalized",
             "type": "column"
            },
            {
             "number": 100,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "diff_normalized",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 1757522429920,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Pandas UDF (vectorized)\n",
    "@pandas_udf(\"double\")\n",
    "def normalized_diff(diff_series):\n",
    "    return (diff_series - diff_series.mean()) / diff_series.std()\n",
    "\n",
    "# Apply both UDFs\n",
    "udf_example = enriched_flights_df \\\n",
    "    .withColumn(\"diff_normalized\", normalized_diff(\"ElapsedTimeDiff\"))\n",
    "\n",
    "display(udf_example)\n",
    "\n",
    "# Note: In practice, prefer built-in functions over UDFs when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9497e8b-22d9-43c3-af61-ed0d4b68d937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## G. Putting it altogether\n",
    "\n",
    "Let's put this together in a chained operation to manipulate data from a source system and save it to a new target (overwriting any existing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ace3a0-3665-4748-a184-1ed5f86387de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop the target table in case it exists already\n",
    "DROP TABLE IF EXISTS cleaned_and_enriched_flights;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb6adc4-3818-4eda-a20b-610cd07760d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substr, lit, when, pandas_udf\n",
    "# or more simply...\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def normalized_diff(diff_series):\n",
    "    return (diff_series - diff_series.mean()) / diff_series.std()\n",
    "\n",
    "(spark.read.table(\"dbacademy_airline.v01.flights_small\")\n",
    "    .selectExpr(\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"DayofMonth\",\n",
    "        \"CAST(DepTime AS INT) AS DepTime\",\n",
    "        \"FlightNum\",\n",
    "        \"CAST(ActualElapsedTime AS INT) AS ActualElapsedTime\",\n",
    "        \"CRSElapsedTime\",\n",
    "        \"CAST(ArrDelay AS INT) AS ArrDelay\"\n",
    "    )\n",
    "    .na.drop()\n",
    "    .withColumn(\n",
    "        \"FlightDateTime\",\n",
    "        make_timestamp_ntz(\n",
    "            col(\"Year\"),\n",
    "            col(\"Month\"),\n",
    "            col(\"DayofMonth\"),\n",
    "            substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(1), lit(2)).cast(\"integer\"),\n",
    "            substr(lpad(col(\"DepTime\"), 4, \"0\"), lit(3), lit(2)).cast(\"integer\"),\n",
    "            lit(0)\n",
    "        )\n",
    "    )\n",
    "    .drop(\"Year\", \"Month\", \"DayofMonth\", \"DepTime\")\n",
    "    .withColumn(\n",
    "        \"ElapsedTimeDiff\", col(\"ActualElapsedTime\") - col(\"CRSElapsedTime\")\n",
    "        )\n",
    "    .drop(\"ActualElapsedTime\", \"CRSElapsedTime\")\n",
    "    .withColumn(\"delay_category\", when(col(\"ArrDelay\") <= 0, \"On Time\")\n",
    "        .when(col(\"ArrDelay\") <= 15, \"Slight Delay\")\n",
    "        .when(col(\"ArrDelay\") <= 60, \"Moderate Delay\")\n",
    "        .otherwise(\"Severe Delay\")) \\\n",
    "    .drop(\"ArrDelay\")\n",
    "    .withColumn(\"diff_normalized\", normalized_diff(\"ElapsedTimeDiff\"))\n",
    "    # Write optimized\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"cleaned_and_enriched_flights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b07b16f-71e6-435c-b1e8-e0f199b4ff9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT * FROM cleaned_and_enriched_flights) SELECT `FlightDateTime`,`diff_normalized` FROM q",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "FlightDateTime",
             "id": "column_e86f7f141108"
            },
            "y": [
             {
              "column": "diff_normalized",
              "id": "column_e86f7f141110"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "box",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "diff_normalized": {
             "type": "box",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "ec2e94f5-c7b7-45e8-9eaa-c97a0b1681f1",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 43.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "FlightDateTime",
           "type": "column"
          },
          {
           "column": "diff_normalized",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM cleaned_and_enriched_flights;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214dee73-04eb-468d-8ab7-6b23fcd8b17e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Data Cleaning Best Practices**:\n",
    "   - Validate and clean data types early\n",
    "   - Handle missing values appropriately\n",
    "   - Document cleaning assumptions\n",
    "\n",
    "2. **Data Enrichment**:\n",
    "   - Create meaningful derived columns\n",
    "   - Consider business requirements\n",
    "   - Use functions (built-in or user defined to enrich datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6814dd5a-e3ea-40ce-a02a-3f95030acff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8057768358105624,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3 - Flight Data ETL with the DataFrame API",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
